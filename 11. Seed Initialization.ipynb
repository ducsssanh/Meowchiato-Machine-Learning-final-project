{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "576a8d17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:11:52.946786Z",
     "iopub.status.busy": "2024-12-16T16:11:52.946496Z",
     "iopub.status.idle": "2024-12-16T16:12:34.242008Z",
     "shell.execute_reply": "2024-12-16T16:12:34.241077Z"
    },
    "papermill": {
     "duration": 41.303749,
     "end_time": "2024-12-16T16:12:34.244217",
     "exception": false,
     "start_time": "2024-12-16T16:11:52.940468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -q install /kaggle/input/pytorchtabnet/pytorch_tabnet-4.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcf3c004",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:12:34.255012Z",
     "iopub.status.busy": "2024-12-16T16:12:34.254722Z",
     "iopub.status.idle": "2024-12-16T16:12:54.148317Z",
     "shell.execute_reply": "2024-12-16T16:12:54.147386Z"
    },
    "papermill": {
     "duration": 19.901415,
     "end_time": "2024-12-16T16:12:54.150444",
     "exception": false,
     "start_time": "2024-12-16T16:12:34.249029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from catboost import CatBoostRegressor\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import clear_output\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from lightgbm import LGBMRegressor\n",
    "from matplotlib.ticker import FormatStrFormatter, MaxNLocator, PercentFormatter\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    VotingRegressor,\n",
    ")\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import (\n",
    "    RepeatedStratifiedKFold,\n",
    "    StratifiedKFold,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b9a9771",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:12:54.160981Z",
     "iopub.status.busy": "2024-12-16T16:12:54.160427Z",
     "iopub.status.idle": "2024-12-16T16:12:54.164414Z",
     "shell.execute_reply": "2024-12-16T16:12:54.163663Z"
    },
    "papermill": {
     "duration": 0.010727,
     "end_time": "2024-12-16T16:12:54.165956",
     "exception": false,
     "start_time": "2024-12-16T16:12:54.155229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e52e7422",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:12:54.177312Z",
     "iopub.status.busy": "2024-12-16T16:12:54.176660Z",
     "iopub.status.idle": "2024-12-16T16:12:54.184699Z",
     "shell.execute_reply": "2024-12-16T16:12:54.184146Z"
    },
    "papermill": {
     "duration": 0.014852,
     "end_time": "2024-12-16T16:12:54.186306",
     "exception": false,
     "start_time": "2024-12-16T16:12:54.171454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ff9b6",
   "metadata": {
    "papermill": {
     "duration": 0.004319,
     "end_time": "2024-12-16T16:12:54.194930",
     "exception": false,
     "start_time": "2024-12-16T16:12:54.190611",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering\n",
    "\n",
    "- **Feature Selection**: The dataset contains features related to physical characteristics (e.g., BMI, Height, Weight), behavioral aspects (e.g., internet usage), and fitness data (e.g., endurance time). \n",
    "- **Categorical Feature Encoding**: Categorical features are mapped to numerical values using custom mappings for each unique category within the dataset. This ensures compatibility with machine learning algorithms that require numerical input.\n",
    "- **Time Series Aggregation**: Time series statistics (e.g., mean, standard deviation) from the actigraphy data are computed and merged into the main dataset to create additional features for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99fb54e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:12:54.204901Z",
     "iopub.status.busy": "2024-12-16T16:12:54.204650Z",
     "iopub.status.idle": "2024-12-16T16:12:54.222170Z",
     "shell.execute_reply": "2024-12-16T16:12:54.221309Z"
    },
    "papermill": {
     "duration": 0.024544,
     "end_time": "2024-12-16T16:12:54.223716",
     "exception": false,
     "start_time": "2024-12-16T16:12:54.199172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_file(filename, dirname):\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    return df.describe().values.reshape(-1), filename.split('=')[1]\n",
    "\n",
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    ids = os.listdir(dirname)\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "    \n",
    "    stats, indexes = zip(*results)\n",
    "    \n",
    "    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "    df['id'] = indexes\n",
    "    return df\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(encoding_dim * 2, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, encoding_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim * 2, input_dim),\n",
    "            nn.Sigmoid()  # For reconstruction in range [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "def perform_autoencoder(df_train, df_test, encoding_dim=50, epochs=100, batch_size=32,patience=10):\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    df_train_scaled = scaler.fit_transform(df_train)\n",
    "    df_test_scaled = scaler.transform(df_train)\n",
    "    \n",
    "    train_tensor = torch.FloatTensor(df_train_scaled)\n",
    "    test_tensor = torch.FloatTensor(df_test_scaled)\n",
    "\n",
    "    # Train-Validation Split\n",
    "    train_tensor, val_tensor = train_test_split(train_tensor.numpy(), test_size=0.2, random_state=42)\n",
    "    train_tensor = torch.FloatTensor(train_tensor)\n",
    "    val_tensor = torch.FloatTensor(val_tensor)\n",
    "    \n",
    "    input_dim = train_tensor.shape[1]\n",
    "    autoencoder = AutoEncoder(input_dim, encoding_dim)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(autoencoder.parameters(), lr=1e-3)\n",
    "\n",
    "    # Learning Rate Scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        autoencoder.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = autoencoder(train_tensor)\n",
    "        loss = criterion(output, train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation step\n",
    "        autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            val_output = autoencoder(val_tensor)\n",
    "            val_loss = criterion(val_output, val_tensor)\n",
    "\n",
    "        # Print losses\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "        # Learning rate adjustment\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(autoencoder.state_dict(), 'best_autoencoder.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Load the best model\n",
    "    autoencoder.load_state_dict(torch.load('best_autoencoder.pth'))\n",
    "\n",
    "    # Encode train and test data\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        train_encoded = autoencoder.encoder(train_tensor).numpy()\n",
    "        test_encoded = autoencoder.encoder(test_tensor).numpy()\n",
    "\n",
    "    return pd.DataFrame(train_encoded, columns=[f'Enc_{i + 1}' for i in range(train_encoded.shape[1])]), pd.DataFrame(test_encoded, columns=[f'Enc_{i + 1}' for i in range(test_encoded.shape[1])])\n",
    "\n",
    "\n",
    "def feature_engineering(df):\n",
    "    season_cols = [col for col in df.columns if 'Season' in col]\n",
    "    df = df.drop(season_cols, axis=1) \n",
    "    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n",
    "    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n",
    "    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n",
    "    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n",
    "    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n",
    "    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n",
    "    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n",
    "    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n",
    "    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n",
    "    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n",
    "    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n",
    "    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n",
    "    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n",
    "    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n",
    "    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6faaa15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:12:54.233283Z",
     "iopub.status.busy": "2024-12-16T16:12:54.233011Z",
     "iopub.status.idle": "2024-12-16T16:14:09.431376Z",
     "shell.execute_reply": "2024-12-16T16:14:09.430360Z"
    },
    "papermill": {
     "duration": 75.205556,
     "end_time": "2024-12-16T16:14:09.433445",
     "exception": false,
     "start_time": "2024-12-16T16:12:54.227889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996/996 [01:12<00:00, 13.68it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 11.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.1012, Val Loss: 0.1006\n",
      "Epoch 2/100, Loss: 0.1004, Val Loss: 0.0998\n",
      "Epoch 3/100, Loss: 0.0995, Val Loss: 0.0988\n",
      "Epoch 4/100, Loss: 0.0986, Val Loss: 0.0978\n",
      "Epoch 5/100, Loss: 0.0976, Val Loss: 0.0967\n",
      "Epoch 6/100, Loss: 0.0965, Val Loss: 0.0955\n",
      "Epoch 7/100, Loss: 0.0952, Val Loss: 0.0940\n",
      "Epoch 8/100, Loss: 0.0937, Val Loss: 0.0924\n",
      "Epoch 9/100, Loss: 0.0919, Val Loss: 0.0904\n",
      "Epoch 10/100, Loss: 0.0900, Val Loss: 0.0882\n",
      "Epoch 11/100, Loss: 0.0876, Val Loss: 0.0855\n",
      "Epoch 12/100, Loss: 0.0849, Val Loss: 0.0825\n",
      "Epoch 13/100, Loss: 0.0818, Val Loss: 0.0790\n",
      "Epoch 14/100, Loss: 0.0783, Val Loss: 0.0752\n",
      "Epoch 15/100, Loss: 0.0744, Val Loss: 0.0711\n",
      "Epoch 16/100, Loss: 0.0703, Val Loss: 0.0669\n",
      "Epoch 17/100, Loss: 0.0661, Val Loss: 0.0627\n",
      "Epoch 18/100, Loss: 0.0618, Val Loss: 0.0587\n",
      "Epoch 19/100, Loss: 0.0578, Val Loss: 0.0551\n",
      "Epoch 20/100, Loss: 0.0544, Val Loss: 0.0523\n",
      "Epoch 21/100, Loss: 0.0514, Val Loss: 0.0502\n",
      "Epoch 22/100, Loss: 0.0493, Val Loss: 0.0487\n",
      "Epoch 23/100, Loss: 0.0477, Val Loss: 0.0478\n",
      "Epoch 24/100, Loss: 0.0467, Val Loss: 0.0469\n",
      "Epoch 25/100, Loss: 0.0459, Val Loss: 0.0459\n",
      "Epoch 26/100, Loss: 0.0449, Val Loss: 0.0448\n",
      "Epoch 27/100, Loss: 0.0438, Val Loss: 0.0435\n",
      "Epoch 28/100, Loss: 0.0426, Val Loss: 0.0421\n",
      "Epoch 29/100, Loss: 0.0415, Val Loss: 0.0409\n",
      "Epoch 30/100, Loss: 0.0402, Val Loss: 0.0399\n",
      "Epoch 31/100, Loss: 0.0394, Val Loss: 0.0391\n",
      "Epoch 32/100, Loss: 0.0388, Val Loss: 0.0386\n",
      "Epoch 33/100, Loss: 0.0383, Val Loss: 0.0382\n",
      "Epoch 34/100, Loss: 0.0381, Val Loss: 0.0380\n",
      "Epoch 35/100, Loss: 0.0380, Val Loss: 0.0379\n",
      "Epoch 36/100, Loss: 0.0380, Val Loss: 0.0379\n",
      "Epoch 37/100, Loss: 0.0379, Val Loss: 0.0378\n",
      "Epoch 38/100, Loss: 0.0379, Val Loss: 0.0377\n",
      "Epoch 39/100, Loss: 0.0378, Val Loss: 0.0376\n",
      "Epoch 40/100, Loss: 0.0377, Val Loss: 0.0374\n",
      "Epoch 41/100, Loss: 0.0375, Val Loss: 0.0372\n",
      "Epoch 42/100, Loss: 0.0373, Val Loss: 0.0370\n",
      "Epoch 43/100, Loss: 0.0370, Val Loss: 0.0367\n",
      "Epoch 44/100, Loss: 0.0368, Val Loss: 0.0365\n",
      "Epoch 45/100, Loss: 0.0365, Val Loss: 0.0363\n",
      "Epoch 46/100, Loss: 0.0363, Val Loss: 0.0362\n",
      "Epoch 47/100, Loss: 0.0361, Val Loss: 0.0361\n",
      "Epoch 48/100, Loss: 0.0360, Val Loss: 0.0360\n",
      "Epoch 49/100, Loss: 0.0359, Val Loss: 0.0360\n",
      "Epoch 50/100, Loss: 0.0358, Val Loss: 0.0359\n",
      "Epoch 51/100, Loss: 0.0357, Val Loss: 0.0359\n",
      "Epoch 52/100, Loss: 0.0357, Val Loss: 0.0358\n",
      "Epoch 53/100, Loss: 0.0356, Val Loss: 0.0357\n",
      "Epoch 54/100, Loss: 0.0356, Val Loss: 0.0356\n",
      "Epoch 55/100, Loss: 0.0355, Val Loss: 0.0354\n",
      "Epoch 56/100, Loss: 0.0353, Val Loss: 0.0353\n",
      "Epoch 57/100, Loss: 0.0351, Val Loss: 0.0351\n",
      "Epoch 58/100, Loss: 0.0350, Val Loss: 0.0349\n",
      "Epoch 59/100, Loss: 0.0349, Val Loss: 0.0347\n",
      "Epoch 60/100, Loss: 0.0347, Val Loss: 0.0345\n",
      "Epoch 61/100, Loss: 0.0345, Val Loss: 0.0343\n",
      "Epoch 62/100, Loss: 0.0344, Val Loss: 0.0341\n",
      "Epoch 63/100, Loss: 0.0342, Val Loss: 0.0338\n",
      "Epoch 64/100, Loss: 0.0341, Val Loss: 0.0336\n",
      "Epoch 65/100, Loss: 0.0338, Val Loss: 0.0334\n",
      "Epoch 66/100, Loss: 0.0336, Val Loss: 0.0331\n",
      "Epoch 67/100, Loss: 0.0334, Val Loss: 0.0328\n",
      "Epoch 68/100, Loss: 0.0331, Val Loss: 0.0324\n",
      "Epoch 69/100, Loss: 0.0328, Val Loss: 0.0321\n",
      "Epoch 70/100, Loss: 0.0326, Val Loss: 0.0317\n",
      "Epoch 71/100, Loss: 0.0321, Val Loss: 0.0313\n",
      "Epoch 72/100, Loss: 0.0319, Val Loss: 0.0309\n",
      "Epoch 73/100, Loss: 0.0315, Val Loss: 0.0304\n",
      "Epoch 74/100, Loss: 0.0312, Val Loss: 0.0300\n",
      "Epoch 75/100, Loss: 0.0307, Val Loss: 0.0294\n",
      "Epoch 76/100, Loss: 0.0303, Val Loss: 0.0289\n",
      "Epoch 77/100, Loss: 0.0299, Val Loss: 0.0284\n",
      "Epoch 78/100, Loss: 0.0293, Val Loss: 0.0278\n",
      "Epoch 79/100, Loss: 0.0291, Val Loss: 0.0273\n",
      "Epoch 80/100, Loss: 0.0285, Val Loss: 0.0267\n",
      "Epoch 81/100, Loss: 0.0280, Val Loss: 0.0261\n",
      "Epoch 82/100, Loss: 0.0274, Val Loss: 0.0256\n",
      "Epoch 83/100, Loss: 0.0271, Val Loss: 0.0250\n",
      "Epoch 84/100, Loss: 0.0266, Val Loss: 0.0245\n",
      "Epoch 85/100, Loss: 0.0261, Val Loss: 0.0240\n",
      "Epoch 86/100, Loss: 0.0257, Val Loss: 0.0236\n",
      "Epoch 87/100, Loss: 0.0254, Val Loss: 0.0231\n",
      "Epoch 88/100, Loss: 0.0249, Val Loss: 0.0227\n",
      "Epoch 89/100, Loss: 0.0243, Val Loss: 0.0222\n",
      "Epoch 90/100, Loss: 0.0242, Val Loss: 0.0218\n",
      "Epoch 91/100, Loss: 0.0239, Val Loss: 0.0215\n",
      "Epoch 92/100, Loss: 0.0233, Val Loss: 0.0211\n",
      "Epoch 93/100, Loss: 0.0230, Val Loss: 0.0207\n",
      "Epoch 94/100, Loss: 0.0229, Val Loss: 0.0203\n",
      "Epoch 95/100, Loss: 0.0225, Val Loss: 0.0199\n",
      "Epoch 96/100, Loss: 0.0219, Val Loss: 0.0195\n",
      "Epoch 97/100, Loss: 0.0216, Val Loss: 0.0192\n",
      "Epoch 98/100, Loss: 0.0213, Val Loss: 0.0188\n",
      "Epoch 99/100, Loss: 0.0207, Val Loss: 0.0184\n",
      "Epoch 100/100, Loss: 0.0206, Val Loss: 0.0181\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n",
    "\n",
    "df_train = train_ts.drop('id', axis=1)\n",
    "df_test = test_ts.drop('id', axis=1)\n",
    "\n",
    "train_ts_encoded, test_ts_encoded = perform_autoencoder(df_train, df_test, encoding_dim=48, epochs=100, batch_size=32)\n",
    "\n",
    "time_series_cols = train_ts_encoded.columns.tolist()\n",
    "train_ts_encoded[\"id\"]=train_ts[\"id\"]\n",
    "test_ts_encoded['id']=test_ts[\"id\"]\n",
    "\n",
    "train = pd.merge(train, train_ts_encoded, how=\"left\", on='id')\n",
    "test = pd.merge(test, test_ts_encoded, how=\"left\", on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "580d6f53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:14:09.475763Z",
     "iopub.status.busy": "2024-12-16T16:14:09.475178Z",
     "iopub.status.idle": "2024-12-16T16:14:16.261815Z",
     "shell.execute_reply": "2024-12-16T16:14:16.261054Z"
    },
    "papermill": {
     "duration": 6.808232,
     "end_time": "2024-12-16T16:14:16.264121",
     "exception": false,
     "start_time": "2024-12-16T16:14:09.455889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KNN\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "numeric_cols = train.select_dtypes(include=['int32', 'float64', 'int64']).columns\n",
    "imputed_data = imputer.fit_transform(train[numeric_cols])\n",
    "train_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n",
    "train_imputed['sii'] = train_imputed['sii'].round().astype(int)\n",
    "for col in train.columns:\n",
    "    if col not in numeric_cols:\n",
    "        train_imputed[col] = train[col]\n",
    "        \n",
    "train = train_imputed\n",
    "\n",
    "train = feature_engineering(train)\n",
    "train = train.dropna(thresh=10, axis=0)\n",
    "test = feature_engineering(test)\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test  = test .drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0a742c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:14:16.303061Z",
     "iopub.status.busy": "2024-12-16T16:14:16.302760Z",
     "iopub.status.idle": "2024-12-16T16:14:16.315165Z",
     "shell.execute_reply": "2024-12-16T16:14:16.314503Z"
    },
    "papermill": {
     "duration": 0.034205,
     "end_time": "2024-12-16T16:14:16.316984",
     "exception": false,
     "start_time": "2024-12-16T16:14:16.282779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-CGAS_Score', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n",
    "                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n",
    "                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW']\n",
    "\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "train = train[featuresCols]\n",
    "train = train.dropna(subset='sii')\n",
    "\n",
    "featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-CGAS_Score', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n",
    "                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n",
    "                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW']\n",
    "\n",
    "featuresCols += time_series_cols\n",
    "test = test[featuresCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e7753dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:14:16.355576Z",
     "iopub.status.busy": "2024-12-16T16:14:16.355293Z",
     "iopub.status.idle": "2024-12-16T16:14:16.363214Z",
     "shell.execute_reply": "2024-12-16T16:14:16.362329Z"
    },
    "papermill": {
     "duration": 0.029255,
     "end_time": "2024-12-16T16:14:16.364878",
     "exception": false,
     "start_time": "2024-12-16T16:14:16.335623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if np.any(np.isinf(train)):\n",
    "    train = train.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ffe916",
   "metadata": {
    "papermill": {
     "duration": 0.017776,
     "end_time": "2024-12-16T16:14:16.400464",
     "exception": false,
     "start_time": "2024-12-16T16:14:16.382688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Training and Evaluation\n",
    "\n",
    "- **Model Types**: Various models are used, including:\n",
    "  - **LightGBM**: A gradient-boosting framework known for its speed and efficiency with large datasets.\n",
    "  - **XGBoost**: Another powerful gradient-boosting model used for structured data.\n",
    "  - **CatBoost**: Optimized for categorical features without the need for extensive preprocessing.\n",
    "  - **Voting Regressor**: An ensemble model that combines the predictions of LightGBM, XGBoost, and CatBoost for better accuracy.\n",
    "- **Cross-Validation**: Stratified K-Folds cross-validation is employed to split the data into training and validation sets, ensuring balanced class distribution in each fold.\n",
    "- **Quadratic Weighted Kappa (QWK)**: The performance of the models is evaluated using QWK, which measures the agreement between predicted and actual values, taking into account the ordinal nature of the target variable.\n",
    "- **Threshold Optimization**: The `minimize` function from `scipy.optimize` is used to fine-tune decision thresholds that map continuous predictions to discrete categories (None, Mild, Moderate, Severe).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408f3b03",
   "metadata": {
    "papermill": {
     "duration": 0.017479,
     "end_time": "2024-12-16T16:14:16.435861",
     "exception": false,
     "start_time": "2024-12-16T16:14:16.418382",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Hyperparameter Tuning\n",
    "\n",
    "- **LightGBM Parameters**: Hyperparameters such as `learning_rate`, `max_depth`, `num_leaves`, and `feature_fraction` are tuned to improve the performance of the LightGBM model. These parameters control the complexity of the model and its ability to generalize to new data.\n",
    "- **XGBoost and CatBoost Parameters**: Similar tuning is applied for XGBoost and CatBoost, adjusting parameters such as `n_estimators`, `max_depth`, `learning_rate`, `subsample`, and `regularization` terms (`reg_alpha`, `reg_lambda`). These help in controlling overfitting and ensuring the model's robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc5fb1",
   "metadata": {
    "papermill": {
     "duration": 0.017696,
     "end_time": "2024-12-16T16:14:16.471257",
     "exception": false,
     "start_time": "2024-12-16T16:14:16.453561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ensemble Learning and Submission Preparation\n",
    "\n",
    "- **Ensemble Learning**: The model uses a **Voting Regressor**, which combines the predictions from LightGBM, XGBoost, and CatBoost. This approach is beneficial as it leverages the strengths of multiple models, reducing overfitting and improving overall model performance.\n",
    "- **Out-of-Fold (OOF) Predictions**: During cross-validation, out-of-fold predictions are generated for the training set, which helps in model evaluation without data leakage.\n",
    "- **Kappa Optimizer**: The Kappa Optimizer ensures that the predicted values are as close to the actual values as possible by adjusting the thresholds used to convert raw model outputs into class labels.\n",
    "- **Test Set Predictions**: After the model is trained and thresholds are optimized, the test dataset is processed, and predictions are generated using the ensemble model. These predictions are converted into the appropriate format for submission.\n",
    "- **Submission File Creation**: The predictions are saved in a CSV file following the required format for submission (e.g., for a Kaggle competition), which includes columns like `id` and `sii` (Severity Impairment Index)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eaad7c",
   "metadata": {
    "papermill": {
     "duration": 0.017367,
     "end_time": "2024-12-16T16:14:16.506394",
     "exception": false,
     "start_time": "2024-12-16T16:14:16.489027",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Final Results and Performance Metrics\n",
    "\n",
    "- **Train and Validation Scores**: After training across multiple folds, the mean Quadratic Weighted Kappa (QWK) score is calculated for both the training and validation datasets, providing an indicator of model performance. \n",
    "- **Optimized QWK Score**: The final optimized QWK score after threshold tuning is displayed, showcasing the model's ability to predict the severity levels effectively.\n",
    "- **Test Predictions**: The test set predictions are evaluated, and a breakdown of the predicted severity levels (None, Mild, Moderate, Severe) is shown, along with their respective counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5207e2cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:14:16.543438Z",
     "iopub.status.busy": "2024-12-16T16:14:16.543114Z",
     "iopub.status.idle": "2024-12-16T16:14:16.554724Z",
     "shell.execute_reply": "2024-12-16T16:14:16.554039Z"
    },
    "papermill": {
     "duration": 0.032178,
     "end_time": "2024-12-16T16:14:16.556376",
     "exception": false,
     "start_time": "2024-12-16T16:14:16.524198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "\n",
    "def adjusted_rounder(preds, thresholds):\n",
    "    preds_rounded = np.digitize(preds, thresholds, right=True)\n",
    "    return preds_rounded.clip(0, 3)  # Ensure predictions are within range\n",
    "\n",
    "def TrainML(model_class, test_data):\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    SKF = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=3, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        if test_preds.shape[1] <= fold:\n",
    "            test_preds = np.hstack([test_preds, np.zeros((len(test_data), 1))])\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    # Dynamic Threshold Optimization\n",
    "    initial_thresholds = np.percentile(oof_non_rounded, [25, 50, 75])\n",
    "    KappaOptimizer = minimize(evaluate_predictions,\n",
    "                              x0=initial_thresholds, args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOptimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = adjusted_rounder(oof_non_rounded, KappaOptimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = adjusted_rounder(tpm, KappaOptimizer.x)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "768fc1a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:14:16.593279Z",
     "iopub.status.busy": "2024-12-16T16:14:16.593006Z",
     "iopub.status.idle": "2024-12-16T16:14:16.606351Z",
     "shell.execute_reply": "2024-12-16T16:14:16.605617Z"
    },
    "papermill": {
     "duration": 0.033569,
     "end_time": "2024-12-16T16:14:16.607784",
     "exception": false,
     "start_time": "2024-12-16T16:14:16.574215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TabNet\n",
    "class TabNetWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = TabNetRegressor(**kwargs)\n",
    "        self.kwargs = kwargs\n",
    "        self.imputer = SimpleImputer(strategy='median')\n",
    "        self.best_model_path = 'best_tabnet_model.pt'\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Handle missing values\n",
    "        X_imputed = self.imputer.fit_transform(X)\n",
    "        \n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "            \n",
    "        # Create internal validation set\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X_imputed, \n",
    "            y, \n",
    "            test_size=0.2,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train TabNet model\n",
    "        history = self.model.fit(\n",
    "            X_train=X_train,\n",
    "            y_train=y_train.reshape(-1, 1),\n",
    "            eval_set=[(X_valid, y_valid.reshape(-1, 1))],\n",
    "            eval_name=['valid'],\n",
    "            eval_metric=['mse'],\n",
    "            max_epochs=200,\n",
    "            patience=20,\n",
    "            batch_size=1024,\n",
    "            virtual_batch_size=128,\n",
    "            num_workers=0,\n",
    "            drop_last=False,\n",
    "            callbacks=[\n",
    "                TabNetPretrainedModelCheckpoint(\n",
    "                    filepath=self.best_model_path,\n",
    "                    monitor='valid_mse',\n",
    "                    mode='min',\n",
    "                    save_best_only=True,\n",
    "                    verbose=True\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Load the best model\n",
    "        if os.path.exists(self.best_model_path):\n",
    "            self.model.load_model(self.best_model_path)\n",
    "            os.remove(self.best_model_path)  # Remove temporary file\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_imputed = self.imputer.transform(X)\n",
    "        return self.model.predict(X_imputed).flatten()\n",
    "    \n",
    "    def __deepcopy__(self, memo):\n",
    "        # Add deepcopy support for scikit-learn\n",
    "        cls = self.__class__\n",
    "        result = cls.__new__(cls)\n",
    "        memo[id(self)] = result\n",
    "        for k, v in self.__dict__.items():\n",
    "            setattr(result, k, deepcopy(v, memo))\n",
    "        return result\n",
    "\n",
    "# TabNet hyperparameters\n",
    "TabNet_Params = {\n",
    "    'n_d': 64,              # Width of the decision prediction layer\n",
    "    'n_a': 64,              # Width of the attention embedding for each step\n",
    "    'n_steps': 5,           # Number of steps in the architecture\n",
    "    'gamma': 1.5,           # Coefficient for feature selection regularization\n",
    "    'n_independent': 2,     # Number of independent GLU layer in each GLU block\n",
    "    'n_shared': 2,          # Number of shared GLU layer in each GLU block\n",
    "    'lambda_sparse': 1e-4,  # Sparsity regularization\n",
    "    'optimizer_fn': torch.optim.Adam,\n",
    "    'optimizer_params': dict(lr=2e-2, weight_decay=1e-5),\n",
    "    'mask_type': 'entmax',\n",
    "    'scheduler_params': dict(mode=\"min\", patience=10, min_lr=1e-5, factor=0.5),\n",
    "    'scheduler_fn': torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    'verbose': 1,\n",
    "    'device_name': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "class TabNetPretrainedModelCheckpoint(Callback):\n",
    "    def __init__(self, filepath, monitor='val_loss', mode='min', \n",
    "                 save_best_only=True, verbose=1):\n",
    "        super().__init__()  # Initialize parent class\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.save_best_only = save_best_only\n",
    "        self.verbose = verbose\n",
    "        self.best = float('inf') if mode == 'min' else -float('inf')\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.model = self.trainer  # Use trainer itself as model\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            return\n",
    "        \n",
    "        # Check if current metric is better than best\n",
    "        if (self.mode == 'min' and current < self.best) or \\\n",
    "           (self.mode == 'max' and current > self.best):\n",
    "            if self.verbose:\n",
    "                print(f'\\nEpoch {epoch}: {self.monitor} improved from {self.best:.4f} to {current:.4f}')\n",
    "            self.best = current\n",
    "            if self.save_best_only:\n",
    "                self.model.save_model(self.filepath)  # Save the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0214392",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:14:16.644274Z",
     "iopub.status.busy": "2024-12-16T16:14:16.643732Z",
     "iopub.status.idle": "2024-12-16T16:14:16.648558Z",
     "shell.execute_reply": "2024-12-16T16:14:16.647764Z"
    },
    "papermill": {
     "duration": 0.024756,
     "end_time": "2024-12-16T16:14:16.650069",
     "exception": false,
     "start_time": "2024-12-16T16:14:16.625313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model parameters for LightGBM\n",
    "Params = {\n",
    "    'learning_rate': 0.046,\n",
    "    'max_depth': 12,\n",
    "    'num_leaves': 478,\n",
    "    'min_data_in_leaf': 13,\n",
    "    'feature_fraction': 0.893,\n",
    "    'bagging_fraction': 0.784,\n",
    "    'bagging_freq': 4,\n",
    "    'lambda_l1': 10,  # Increased from 6.59\n",
    "    'lambda_l2': 0.01,  # Increased from 2.68e-06\n",
    "    'device': 'cpu'\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# XGBoost parameters\n",
    "XGB_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1,  # Increased from 0.1\n",
    "    'reg_lambda': 5,  # Increased from 1\n",
    "    'random_state': SEED,\n",
    "    'tree_method': 'gpu_hist',\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "CatBoost_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10,  # Increase this value\n",
    "    'task_type': 'GPU'\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1301549f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:14:16.686250Z",
     "iopub.status.busy": "2024-12-16T16:14:16.685775Z",
     "iopub.status.idle": "2024-12-16T16:14:16.695548Z",
     "shell.execute_reply": "2024-12-16T16:14:16.694975Z"
    },
    "papermill": {
     "duration": 0.029579,
     "end_time": "2024-12-16T16:14:16.697039",
     "exception": false,
     "start_time": "2024-12-16T16:14:16.667460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model instances\n",
    "Light = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\n",
    "XGB_Model = XGBRegressor(**XGB_Params)\n",
    "CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
    "TabNet_Model = TabNetWrapper(**TabNet_Params) # New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eec8e733",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:14:16.733643Z",
     "iopub.status.busy": "2024-12-16T16:14:16.733396Z",
     "iopub.status.idle": "2024-12-16T16:18:49.374978Z",
     "shell.execute_reply": "2024-12-16T16:18:49.373867Z"
    },
    "papermill": {
     "duration": 272.662001,
     "end_time": "2024-12-16T16:18:49.377066",
     "exception": false,
     "start_time": "2024-12-16T16:14:16.715065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 15it [04:32, 18.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK --> 0.7280\n",
      "Mean Validation QWK ---> 0.4809\n",
      "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.531\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    1\n",
       "1   000fd460    0\n",
       "2   00105258    1\n",
       "3   00115b9f    0\n",
       "4   0016bb22    1\n",
       "5   001f3379    1\n",
       "6   0038ba98    1\n",
       "7   0068a485    1\n",
       "8   0069fbed    1\n",
       "9   0083e397    1\n",
       "10  0087dd65    1\n",
       "11  00abe655    1\n",
       "12  00ae59c9    1\n",
       "13  00af6387    1\n",
       "14  00bd4359    1\n",
       "15  00c0cd71    1\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    1\n",
       "18  00e6167c    1\n",
       "19  00ebc35d    1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_model = VotingRegressor(estimators=[\n",
    "    ('lightgbm', Light),\n",
    "    ('xgboost', XGB_Model),\n",
    "    ('catboost', CatBoost_Model),\n",
    "    ('tabnet', TabNet_Model)\n",
    "])\n",
    "\n",
    "Submission1 = TrainML(voting_model, test)\n",
    "\n",
    "Submission1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae6e4f41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:18:49.416823Z",
     "iopub.status.busy": "2024-12-16T16:18:49.416571Z",
     "iopub.status.idle": "2024-12-16T16:20:51.175276Z",
     "shell.execute_reply": "2024-12-16T16:20:51.174412Z"
    },
    "papermill": {
     "duration": 121.780059,
     "end_time": "2024-12-16T16:20:51.177006",
     "exception": false,
     "start_time": "2024-12-16T16:18:49.396947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [00:50<00:00, 10.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK --> 0.7595\n",
      "Mean Validation QWK ---> 0.3926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.457\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    1\n",
       "1   000fd460    0\n",
       "2   00105258    0\n",
       "3   00115b9f    0\n",
       "4   0016bb22    0\n",
       "5   001f3379    1\n",
       "6   0038ba98    0\n",
       "7   0068a485    0\n",
       "8   0069fbed    1\n",
       "9   0083e397    0\n",
       "10  0087dd65    0\n",
       "11  00abe655    0\n",
       "12  00ae59c9    1\n",
       "13  00af6387    1\n",
       "14  00bd4359    1\n",
       "15  00c0cd71    1\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    0\n",
       "18  00e6167c    0\n",
       "19  00ebc35d    1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "def process_file(filename, dirname):\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    return df.describe().values.reshape(-1), filename.split('=')[1]\n",
    "\n",
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    ids = os.listdir(dirname)\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "    \n",
    "    stats, indexes = zip(*results)\n",
    "    \n",
    "    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "    df['id'] = indexes\n",
    "    return df\n",
    "        \n",
    "train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n",
    "\n",
    "time_series_cols = train_ts.columns.tolist()\n",
    "time_series_cols.remove(\"id\")\n",
    "\n",
    "train = pd.merge(train, train_ts, how=\"left\", on='id')\n",
    "test = pd.merge(test, test_ts, how=\"left\", on='id')\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)   \n",
    "\n",
    "featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n",
    "\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "train = train[featuresCols]\n",
    "train = train.dropna(subset='sii')\n",
    "\n",
    "cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n",
    "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n",
    "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n",
    "\n",
    "def update(df):\n",
    "    global cat_c\n",
    "    for c in cat_c: \n",
    "        df[c] = df[c].fillna('Missing')\n",
    "        df[c] = df[c].astype('category')\n",
    "    return df\n",
    "        \n",
    "train = update(train)\n",
    "test = update(test)\n",
    "\n",
    "def create_mapping(column, dataset):\n",
    "    unique_values = dataset[column].unique()\n",
    "    return {value: idx for idx, value in enumerate(unique_values)}\n",
    "\n",
    "for col in cat_c:\n",
    "    mapping = create_mapping(col, train)\n",
    "    mappingTe = create_mapping(col, test)\n",
    "    \n",
    "    train[col] = train[col].replace(mapping).astype(int)\n",
    "    test[col] = test[col].replace(mappingTe).astype(int)\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "\n",
    "def TrainML(model_class, test_data):\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return submission\n",
    "\n",
    "# Model parameters for LightGBM\n",
    "Params = {\n",
    "    'learning_rate': 0.046,\n",
    "    'max_depth': 12,\n",
    "    'num_leaves': 478,\n",
    "    'min_data_in_leaf': 13,\n",
    "    'feature_fraction': 0.893,\n",
    "    'bagging_fraction': 0.784,\n",
    "    'bagging_freq': 4,\n",
    "    'lambda_l1': 10,  # Increased from 6.59\n",
    "    'lambda_l2': 0.01  # Increased from 2.68e-06\n",
    "}\n",
    "\n",
    "\n",
    "# XGBoost parameters\n",
    "XGB_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1,  # Increased from 0.1\n",
    "    'reg_lambda': 5,  # Increased from 1\n",
    "    'random_state': SEED\n",
    "}\n",
    "\n",
    "\n",
    "CatBoost_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': SEED,\n",
    "    'cat_features': cat_c,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10  # Increase this value\n",
    "}\n",
    "\n",
    "# Create model instances\n",
    "Light = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\n",
    "XGB_Model = XGBRegressor(**XGB_Params)\n",
    "CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
    "\n",
    "# Combine models using Voting Regressor\n",
    "voting_model = VotingRegressor(estimators=[\n",
    "    ('lightgbm', Light),\n",
    "    ('xgboost', XGB_Model),\n",
    "    ('catboost', CatBoost_Model)\n",
    "])\n",
    "\n",
    "# Train the ensemble model\n",
    "Submission2 = TrainML(voting_model, test)\n",
    "\n",
    "# Save submission\n",
    "#Submission2.to_csv('submission.csv', index=False)\n",
    "Submission2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e22b9381",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:20:51.215417Z",
     "iopub.status.busy": "2024-12-16T16:20:51.215118Z",
     "iopub.status.idle": "2024-12-16T16:24:03.859463Z",
     "shell.execute_reply": "2024-12-16T16:24:03.858423Z"
    },
    "papermill": {
     "duration": 192.665886,
     "end_time": "2024-12-16T16:24:03.861379",
     "exception": false,
     "start_time": "2024-12-16T16:20:51.195493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [02:01<00:00, 24.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK --> 0.9175\n",
      "Mean Validation QWK ---> 0.3803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.450\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    2\n",
       "1   000fd460    0\n",
       "2   00105258    0\n",
       "3   00115b9f    0\n",
       "4   0016bb22    1\n",
       "5   001f3379    1\n",
       "6   0038ba98    0\n",
       "7   0068a485    0\n",
       "8   0069fbed    2\n",
       "9   0083e397    0\n",
       "10  0087dd65    1\n",
       "11  00abe655    0\n",
       "12  00ae59c9    2\n",
       "13  00af6387    1\n",
       "14  00bd4359    2\n",
       "15  00c0cd71    2\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    0\n",
       "18  00e6167c    0\n",
       "19  00ebc35d    1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n",
    "\n",
    "cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n",
    "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n",
    "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n",
    "\n",
    "train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n",
    "\n",
    "time_series_cols = train_ts.columns.tolist()\n",
    "time_series_cols.remove(\"id\")\n",
    "\n",
    "train = pd.merge(train, train_ts, how=\"left\", on='id')\n",
    "test = pd.merge(test, test_ts, how=\"left\", on='id')\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)\n",
    "\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "train = train[featuresCols]\n",
    "train = train.dropna(subset='sii')\n",
    "\n",
    "def update(df):\n",
    "    global cat_c\n",
    "    for c in cat_c: \n",
    "        df[c] = df[c].fillna('Missing')\n",
    "        df[c] = df[c].astype('category')\n",
    "    return df\n",
    "\n",
    "train = update(train)\n",
    "test = update(test)\n",
    "\n",
    "def create_mapping(column, dataset):\n",
    "    unique_values = dataset[column].unique()\n",
    "    return {value: idx for idx, value in enumerate(unique_values)}\n",
    "\n",
    "for col in cat_c:\n",
    "    mapping = create_mapping(col, train)\n",
    "    mappingTe = create_mapping(col, test)\n",
    "    \n",
    "    train[col] = train[col].replace(mapping).astype(int)\n",
    "    test[col] = test[col].replace(mappingTe).astype(int)\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "\n",
    "def TrainML(model_class, test_data):\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tp_rounded = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "\n",
    "    return tp_rounded\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "ensemble = VotingRegressor(estimators=[\n",
    "    ('lgb', Pipeline(steps=[('imputer', imputer), ('regressor', LGBMRegressor(random_state=SEED))])),\n",
    "    ('xgb', Pipeline(steps=[('imputer', imputer), ('regressor', XGBRegressor(random_state=SEED))])),\n",
    "    ('cat', Pipeline(steps=[('imputer', imputer), ('regressor', CatBoostRegressor(random_state=SEED, silent=True))])),\n",
    "    ('rf', Pipeline(steps=[('imputer', imputer), ('regressor', RandomForestRegressor(random_state=SEED))])),\n",
    "    ('gb', Pipeline(steps=[('imputer', imputer), ('regressor', GradientBoostingRegressor(random_state=SEED))]))\n",
    "])\n",
    "\n",
    "Submission3 = TrainML(ensemble, test)\n",
    "Submission3 = pd.DataFrame({\n",
    "    'id': sample['id'],\n",
    "    'sii': Submission3\n",
    "})\n",
    "\n",
    "Submission3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cb54c6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:24:03.903924Z",
     "iopub.status.busy": "2024-12-16T16:24:03.903273Z",
     "iopub.status.idle": "2024-12-16T16:24:03.921290Z",
     "shell.execute_reply": "2024-12-16T16:24:03.920341Z"
    },
    "papermill": {
     "duration": 0.040225,
     "end_time": "2024-12-16T16:24:03.923474",
     "exception": false,
     "start_time": "2024-12-16T16:24:03.883249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority voting completed and saved to 'Final_Submission.csv'\n"
     ]
    }
   ],
   "source": [
    "sub1 = Submission1\n",
    "sub2 = Submission2\n",
    "sub3 = Submission3\n",
    "\n",
    "sub1 = sub1.sort_values(by='id').reset_index(drop=True)\n",
    "sub2 = sub2.sort_values(by='id').reset_index(drop=True)\n",
    "sub3 = sub3.sort_values(by='id').reset_index(drop=True)\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'id': sub1['id'],\n",
    "    'sii_1': sub1['sii'],\n",
    "    'sii_2': sub2['sii'],\n",
    "    'sii_3': sub3['sii']\n",
    "})\n",
    "\n",
    "def majority_vote(row):\n",
    "    return row.mode()[0]\n",
    "\n",
    "combined['final_sii'] = combined[['sii_1', 'sii_2', 'sii_3']].apply(majority_vote, axis=1)\n",
    "\n",
    "final_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n",
    "\n",
    "final_submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Majority voting completed and saved to 'Final_Submission.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef668ebf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T16:24:03.963397Z",
     "iopub.status.busy": "2024-12-16T16:24:03.962835Z",
     "iopub.status.idle": "2024-12-16T16:24:03.970517Z",
     "shell.execute_reply": "2024-12-16T16:24:03.969800Z"
    },
    "papermill": {
     "duration": 0.028566,
     "end_time": "2024-12-16T16:24:03.972022",
     "exception": false,
     "start_time": "2024-12-16T16:24:03.943456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    1\n",
       "1   000fd460    0\n",
       "2   00105258    0\n",
       "3   00115b9f    0\n",
       "4   0016bb22    1\n",
       "5   001f3379    1\n",
       "6   0038ba98    0\n",
       "7   0068a485    0\n",
       "8   0069fbed    1\n",
       "9   0083e397    0\n",
       "10  0087dd65    1\n",
       "11  00abe655    0\n",
       "12  00ae59c9    1\n",
       "13  00af6387    1\n",
       "14  00bd4359    1\n",
       "15  00c0cd71    1\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    0\n",
       "18  00e6167c    0\n",
       "19  00ebc35d    1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_submission"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    },
    {
     "datasetId": 921302,
     "sourceId": 7453542,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 737.140536,
   "end_time": "2024-12-16T16:24:07.597625",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-16T16:11:50.457089",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
